---
title: "Detecting credit defaulters using logistic regression"
output:
  word_document: default
  html_notebook: default
---
#Logistic Regression analysis of the credit data

```{r}

#https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls
#Step 1: Collect data
credit <- read.csv("default_of_credit_card_clients_1.csv")

#In the csv file used, we discard column names X1, X2... to more meaningful names given in row2- LIMIT_BAL, SEX.. present in the second row
#remove the ID variable to avoid overfitting and remove factor names additionally present in row
credit<-credit[,-1]
```
```{r}
#Step 2: exploring the data
str(credit)
table(credit$default.payment.next.month)
prop.table(table(credit$default.payment.next.month))
```
#Our credit dataset consists of 30000 observations and 23 factors along with 1 factor for classification type.
#6636 customers (22.12%) are defaulters and 23364 customers (77.88%) are not defaulters
```{r}
#preparing the data:

#preparing test and train data 80%-20%
credit_train <- credit[1:24000,]
credit_test <- credit[24001:30000,]

credit_train_labels <- credit[1:24000,]$default.payment.next.month
credit_test_labels <- credit[24001:30000,]$default.payment.next.month

prop.table(table(credit_train$default.payment.next.month))
prop.table(table(credit_test$default.payment.next.month))
```
#The proportion od defaulters in training and test datasets is almost the same
#We now check our data for any missing values
```{r}
library(Amelia)
missmap(credit, main = "Missing values vs observed")
sapply(credit,function(x) sum(is.na(x)))
```
#From the plot and the table, we see that there are no missing values in our data
```{r}
#see number of unique values per feature
sapply(credit, function(x) length(unique(x)))
```
```{r}
#Step 3: Training the Model
model <- glm(default.payment.next.month ~.,family=binomial(link='logit'),data=credit_train)
model
```

```{r}
summary(model)
```

```{r}
anova(model, test="Chisq")
```

```{r}
#Step 4: Evaluate the model
fitted.results <- predict(model,newdata=credit_test,type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)

misClasificError <- mean(fitted.results != credit_test$default.payment.next.month)
print(paste('Accuracy',1-misClasificError))

```
#The accuracy of our model is 81.92% (Misclassification rate is 18.08%)
```{r}
library(ROCR)
p <- predict(model, newdata=credit_test, type="response")
pr <- prediction(p, credit_test$default.payment.next.month)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
```
#From the ROC Curve we have a value of 0.7323 via the AUC function

#Step 5: Improving the Model
#To improve our model, we drop the insignificant predictors by specifying an alpha value of 0.10 The new model will now use the features:  
```{r}
model2 <- glm(default.payment.next.month ~ LIMIT_BAL +SEX+ EDUCATION+ MARRIAGE + AGE +  PAY_0 +PAY_2+ PAY_3 + PAY_5 + BILL_AMT1 +BILL_AMT2 + PAY_AMT1 + PAY_AMT2+ PAY_AMT3+ PAY_AMT6 ,family=binomial(link='logit'),data=credit_train)
model2
```
```{r}
summary(model2)
```
```{r}
fitted.results2 <- predict(model2,newdata=credit_test,type='response')
fitted.results2 <- ifelse(fitted.results2 > 0.5,1,0)
misClasificError2 <- mean(fitted.results2 != credit_test$default.payment.next.month)
print(paste('Accuracy',1-misClasificError2))

p2 <- predict(model2, newdata=credit_test, type="response")
pr2 <- prediction(p2, credit_test$default.payment.next.month)
prf2 <- performance(pr2, measure = "tpr", x.measure = "fpr")
plot(prf2)
auc1 <- performance(pr2, measure = "auc")
auc1 <- auc1@y.values[[1]]
auc1
```
#The improved model now gives an Accuracy of 81.97%. From the ROC Curve we have a value of 0.7317 via the AUC function. 
This is a slight improvement.

